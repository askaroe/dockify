{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DeepSeek LoRA Fine-tuning\n",
        "\n",
        "Model: DeepSeek-R1-Distill-Qwen-1.5B\n",
        "Framework: MindNLP + MindSpore\n",
        "Dataset: AI Medical Chatbot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q mindspore==2.3.1\n",
        "%pip install -q mindnlp\n",
        "%pip install -q transformers==4.44.0\n",
        "%pip install -q peft==0.12.0\n",
        "%pip install -q datasets==2.19.0\n",
        "%pip install -q accelerate==0.30.0\n",
        "%pip install -q trl==0.8.6\n",
        "%pip install -q sentencepiece\n",
        "%pip install -q protobuf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import mindspore\n",
        "import mindnlp\n",
        "from mindspore import context\n",
        "import pandas as pd\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "\n",
        "context.set_context(mode=context.PYNATIVE_MODE, device_target=\"GPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('/kaggle/input/ai-medical-chatbot/ai_medical_chatbot.csv')\n",
        "dataset = Dataset.from_pandas(df)\n",
        "print(f\"Loaded {len(dataset)} examples\")\n",
        "print(dataset[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df = pd.read_csv('/kaggle/input/gym-exercise-data/gym_exercise_data.csv')\n",
        "# dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# df = pd.read_csv('/kaggle/input/yoga-poses-dataset/yoga_poses.csv')\n",
        "# dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# dataset = load_dataset(\"BI55/MedText\", split=\"train\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Format Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_instruction(example):\n",
        "    if 'question' in example and 'answer' in example:\n",
        "        question = example['question']\n",
        "        answer = example['answer']\n",
        "    elif 'instruction' in example and 'output' in example:\n",
        "        question = example['instruction']\n",
        "        answer = example['output']\n",
        "    elif 'input' in example and 'output' in example:\n",
        "        question = example['input']\n",
        "        answer = example['output']\n",
        "    else:\n",
        "        cols = list(example.keys())\n",
        "        question = example[cols[0]]\n",
        "        answer = example[cols[1]]\n",
        "    \n",
        "    text = f\"\"\"### Instruction:\n",
        "You are a medical and sports health assistant. Answer the following question accurately and helpfully.\n",
        "\n",
        "### Question:\n",
        "{question}\n",
        "\n",
        "### Answer:\n",
        "{answer}\"\"\"\n",
        "    \n",
        "    return {\"text\": text}\n",
        "\n",
        "formatted_dataset = dataset.map(format_instruction, remove_columns=dataset.column_names)\n",
        "print(f\"Formatted {len(formatted_dataset)} examples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_test_split = formatted_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = train_test_split['train']\n",
        "eval_dataset = train_test_split['test']\n",
        "\n",
        "MAX_TRAIN_SAMPLES = 1000\n",
        "MAX_EVAL_SAMPLES = 100\n",
        "\n",
        "if MAX_TRAIN_SAMPLES and len(train_dataset) > MAX_TRAIN_SAMPLES:\n",
        "    train_dataset = train_dataset.select(range(MAX_TRAIN_SAMPLES))\n",
        "if MAX_EVAL_SAMPLES and len(eval_dataset) > MAX_EVAL_SAMPLES:\n",
        "    eval_dataset = eval_dataset.select(range(MAX_EVAL_SAMPLES))\n",
        "\n",
        "print(f\"Train: {len(train_dataset)}, Eval: {len(eval_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=mindspore.float16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure LoRA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./deepseek-mindnlp-medical\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=False,\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    warmup_steps=50,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    gradient_checkpointing=True,\n",
        "    group_by_length=True,\n",
        "    report_to=\"none\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    packing=False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_dir = \"./deepseek-mindnlp-medical-final\"\n",
        "trainer.model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "print(f\"Saved to: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_answer(question, max_length=256):\n",
        "    prompt = f\"\"\"### Instruction:\n",
        "You are a medical and sports health assistant. Answer the following question accurately and helpfully.\n",
        "\n",
        "### Question:\n",
        "{question}\n",
        "\n",
        "### Answer:\n",
        "\"\"\"\n",
        "    \n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_length,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    \n",
        "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    if \"### Answer:\" in full_response:\n",
        "        answer = full_response.split(\"### Answer:\")[1].strip()\n",
        "    else:\n",
        "        answer = full_response\n",
        "    \n",
        "    return answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_questions = [\n",
        "    \"What are the health benefits of regular exercise?\",\n",
        "    \"How many hours of sleep do adults need per night?\",\n",
        "    \"What is a normal resting heart rate?\",\n",
        "    \"How can I improve my sleep quality?\",\n",
        "    \"What is BMI and why does it matter?\"\n",
        "]\n",
        "\n",
        "for question in test_questions:\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"A: {generate_answer(question)}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "kaggle_output = \"/kaggle/working/deepseek-mindnlp-medical\"\n",
        "os.makedirs(kaggle_output, exist_ok=True)\n",
        "\n",
        "for file in os.listdir(output_dir):\n",
        "    src = os.path.join(output_dir, file)\n",
        "    dst = os.path.join(kaggle_output, file)\n",
        "    if os.isfile(src):\n",
        "        shutil.copy2(src, dst)\n",
        "\n",
        "print(f\"Exported to: {kaggle_output}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
